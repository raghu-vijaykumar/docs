# AWS Kinesis Fundamentals

## Introduction
AWS Kinesis is a core service for developing stream processing applications, enabling real-time processing of data from multiple producers and consumers. Producers such as mobile devices, servers, and IoT devices continuously write data to Kinesis, while consumers like dashboards, databases, or analytic applications process this data in real-time.

### Use Cases
- **Log Collection and Processing**: Application logs can be collected and processed in real-time.
- **Message Passing**: Send messages between applications.
- **Metrics Tracking**: Track real-time metrics generated by applications or devices.
- **User Activity Analysis**: Analyze and react to user activity in real-time.

### Key Features
1. **Real-Time Performance**: Low latency between data generation and consumption.
2. **High Throughput**: Scalable to process massive amounts of data, e.g., AdRoll processes 1.3PB/month.
3. **Elasticity**: Scales up or down based on demand.
4. **Integration with AWS Services**: Seamlessly works with other AWS tools.
5. **Cost-Efficiency**: For example, streaming 1 MB/sec costs just 1 cent per hour.
6. **Low Administration Overhead**: It is serverless, reducing the need for management of infrastructure.

## History of Kinesis
Before stream processing, batch processing was the standard, which involved producers aggregating data and storing it in distributed storage (like S3 or HDFS). Consumers would then process this data with significant latency (e.g., hours or days). For use cases like real-time fraud detection, financial trading, and clickstream analysis, batch processing was insufficient.

### Evolution of Stream Processing
- **2008**: LinkedIn developed an internal data pipeline to handle high-latency batch processing challenges.
- **2011**: Apache Kafka was released by LinkedIn as a real-time streaming solution.
- **2013**: AWS launched Kinesis, inspired by Kafka.
- **2014**: Google released Cloud Dataflow.
- **2015**: AWS introduced Kinesis Firehose to deliver data to storage like S3 and Redshift.
- **2016**: AWS added Kinesis Analytics, allowing SQL queries over streaming data.

## Kinesis Building Blocks

### Streams
At the core of Kinesis is an immutable log of ordered, binary records, each with a sequence number. Key properties include:
- **Append-Only**: New records are always appended; existing records are immutable.
- **Ordered Reading**: All consumers read records in the same order.
- **Multiple Consumers**: Multiple consumers can read the same log concurrently, without impacting other readers.
- **Retention**: Data is retained for 24 hours by default, after which it is automatically removed.

### Shards
Kinesis scales through shards:
- **Shard**: A unit of scale in Kinesis that holds a partition of the overall log.
- **Scalability**: You can add more shards to increase throughput. Shards can be distributed across multiple machines.
- **Record Distribution**: Kinesis uses the key of a record and an MD5 hash to assign records to shards, allowing for load balancing and ensuring all records with the same key go to the same shard.

### Key Selection Strategies
- **Random Keys**: Used for balancing data across shards.
- **Meaningful Keys**: Used when key-based processing is important. For instance, tweets with the same hashtag could be processed together by assigning the hashtag as the key.

### Replication and Availability
Each shard is replicated across three machines in different availability zones, ensuring data durability.

### Consumer Responsibilities
Consumers must keep track of their position in the log and store this position in a durable storage system. This ensures that, in the event of a failure, consumers can resume processing from where they left off.

## Comparison to Other Technologies

### Kinesis vs. Apache Kafka
- **Inspiration**: Kinesis was inspired by Apache Kafka.
- **Architecture**: Both are messaging systems designed for real-time stream processing.
  
### Kinesis vs. AWS SQS
- **Messaging Focus**: While both are AWS messaging services, SQS is designed more for queueing individual messages, while Kinesis is designed for continuous stream processing.

# Reading and Writing Data to Kinesis

## Introduction

This document covers the process of reading and writing data to **Amazon Kinesis**, focusing on the usage of its API to implement both **producers** and **consumers** for data streams.

---

## Kinesis API Overview

Kinesis API enables users to interact with Kinesis streams for managing and processing data. The API is divided into multiple functionalities:

### Stream Management
- **CreateStream**: Creates a Kinesis stream.
- **DeleteStream**: Deletes a stream.
- **ListStreams**: Lists available streams in the account.
- **DescribeStream**: Provides information about a specific stream, including its shards.
  
### Shard Management
Kinesis does not allow direct manipulation of individual shards (adding or deleting shards). Instead:
- **MergeShards**: Merges two shards into one.
- **SplitShard**: Splits a shard into two.

These operations enable users to scale the stream up or down by changing the number of shards.

### Stream Configuration
- **IncreaseRetentionPeriod**: Changes the default retention period (24 hours).
- **EnableEnhancedMonitoring**: Enables monitoring features.
- **EnableStreamEncryption**: Enables encryption for data security.

### Writing Data
- **PutRecord**: Writes a single record to a stream.
- **PutRecords**: Writes multiple records to a stream in a single operation.

### Reading Data
There are two main ways to read data from Kinesis:
1. **Shard Iterator**: Periodically polls records from a stream using an iterator.
2. **Fan-Out Consumers**: Allows Kinesis to push records to consumers as soon as they are available.

---

## Writing Data to Kinesis

This section describes how to implement a Kinesis producer to write data to a stream. For demonstration purposes, Twitter data is used, where tweets are written to a Kinesis stream.

### Steps:
1. **Create a Kinesis Stream**:
   - Go to AWS Console → Kinesis → Create Data Stream.
   - Provide a stream name (e.g., `tweets-stream`).
   - Define the number of shards for the stream.
  
2. **Configure the Application**:
   - Add a dependency on **AWS SDK** to interact with AWS services.
   - Create a Kinesis client using the SDK.
   - For each tweet, generate a **byte array** representing the tweet and prepare it for the Kinesis stream using **PutRecordRequest**.

3. **Partitioning Tweets**:
   - The partition key (e.g., language of the tweet) determines which shard the data will go to. Tweets in the same language will go to the same shard.

4. **Run the Producer**:
   - The producer application sends records to the stream, and you can observe which shards the tweets are written to based on their language (e.g., English tweets to shard 1, Spanish tweets to shard 0).

---

## Reading Data from Kinesis

Once a producer is writing data to a Kinesis stream, a **consumer** application can be implemented to read and process the data in real-time.

### Steps:
1. **Get Shards Information**:
   - The consumer application starts by sending a **DescribeStream** request to get the list of shards in the stream.

2. **Get Shard Iterator**:
   - Use the **GetShardIterator** method to get an iterator for a specific shard. The iterator determines from which position in the shard to start reading (e.g., from the latest record or the beginning of the stream).

3. **Read Records**:
   - Use the **GetRecords** method with the shard iterator to retrieve records from the stream. This method returns a batch of records along with the next shard iterator for continuous reading.
   
4. **Process Records**:
   - Each record retrieved contains data (e.g., tweets in JSON format). The consumer processes the data and displays relevant information, such as the tweet’s text and language.
   
5. **Loop to Continuously Read Data**:
   - The consumer enters a loop where it fetches records from Kinesis, processes them, sleeps for a defined interval (e.g., 200 ms), and fetches the next batch using the updated shard iterator.

6. **Handling Multiple Shards**:
   - The application can be configured to read from multiple shards by managing multiple shard iterators, ensuring all data from the stream is processed.

---

