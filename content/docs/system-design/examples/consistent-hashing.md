+++
title= "Estimation"
tags = [ "system-design", "software-architecture", "interview", "estimation" ]
author = "Me"
showToc = true
TocOpen = false
draft = false
hidemeta = false
comments = false
disableShare = false
disableHLJS = false
hideSummary = false
searchHidden = true
ShowReadingTime = true
ShowBreadCrumbs = true
ShowPostNavLinks = true
ShowWordCount = true
ShowRssButtonInSectionTermList = true
UseHugoToc = true
weight= 4
bookFlatSection= true
+++

# Design Consistent Hashing

When building horizontally scalable systems, distributing requests efficiently across servers is crucial. One of the most common techniques to achieve this is **consistent hashing**.

---

## The Rehashing Problem

A common way to distribute requests to servers is by using a simple hash and modulo formula:

`serverIndex = hash(key) % N`, where N is the number of servers
This method ensures requests are distributed uniformly across all servers. However, when new servers are added or existing ones are removed, the results of this equation change significantly. As a result, many requests will be rerouted to different servers, causing a large number of cache misses, as clients connect to new instances that need to fetch user data from cache again.

---

## Consistent Hashing

**Consistent hashing** is a technique designed to minimize the number of re-mappings that occur when servers are added or removed. Instead of re-mapping all keys, only **K/N** servers are remapped when the total number of servers **N** changes. Here, **K** is the total number of keys.

For example:
- With **K = 100** and **N = 10**, only 10 keys will be re-mapped, compared to nearly 100 in the regular scenario.

---

## Hash Space and Hash Ring

A **hash ring** is a representation of the key space generated by a hash algorithm, visualized as a circular structure (a ring).

![hash-ring](../images/hash-ring.png)

---

## Hash Servers

By applying the same hash function to the servers (based on server IP or name), we map the servers onto this hash ring.

![hash-servers](../images/hash-servers.png)

---

## Hash Keys

Similarly, the hashes of the requests are also mapped onto the hash ring. Unlike the modulo approach, here the keys are distributed based on the hash function alone.

![hash-keys](../images/hash-keys.png)

---

## Server Lookup

To find out which server should serve a request, we move **clockwise** from the requestâ€™s hash on the ring until we find the first server hash. This server will handle the request.

![server-lookup](../images/server-lookup.png)

---

## Adding a Server

When a new server is added, only the keys immediately adjacent (in the hash ring) will be remapped to the new server. This keeps the system efficient, as only a fraction of the keys are affected.

![add-server-scenario](../images/add-server-scenario.png)

---

## Removing a Server

Similarly, when a server is removed, only the keys adjacent to the removed server will be remapped to other servers.

![remove-server-scenario](../images/remove-server-scenario.png)

---

## Two Issues in the Basic Approach

1. **Uneven Hash Partitions**: Hash partitions can be distributed unevenly across servers.
   ![hash-partitions](../images/hash-partitions.png)

2. **Uneven Request Distribution**: As a result of the uneven partitioning, the requests might not be evenly distributed, causing load imbalance.
   ![uneven-request-distribution](../images/uneven-request-distribution.png)

---

## Virtual Nodes

To resolve the problem of uneven distribution, we use **virtual nodes**. Servers are mapped onto the hash ring multiple times, creating virtual nodes that represent each server at multiple points on the ring.

![virtual-nodes](../images/virtual-nodes.png)

With virtual nodes, a request is mapped to the nearest virtual node, ensuring a more even distribution of requests.

![virtual-node-request-mapping](../images/virtual-node-request-mapping.png)

The more virtual nodes there are, the more evenly the requests are distributed. Studies have shown that having between 100 to 200 virtual nodes leads to a **standard deviation of 5-10%** in request distribution.

---

## Wrap Up

**Benefits of Consistent Hashing**:
- Minimal key remapping during rebalancing events.
- Simplifies horizontal scaling as data is uniformly distributed.
- Reduces the risk of hotspots by evenly distributing requests, even for popular entities like celebrities.

**Real-world Applications of Consistent Hashing**:
- [Amazon's DynamoDB partitioning](https://aws.amazon.com/dynamodb/)
- [Data partitioning in Cassandra](https://cassandra.apache.org/)
- [Discord chat application](https://discord.com/)
- [Akamai CDN](https://www.akamai.com/)
- [Maglev network load balancer](https://cloud.google.com/blog/products/networking/meet-maglev-google-global-load-balancer)
